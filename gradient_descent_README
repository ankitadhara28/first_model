# Gradient Descent for Linear Regression

A from-scratch implementation of gradient descent to find the best-fit line for data points by minimizing prediction error.

## Theory

**Objective**: Find optimal parameters (m, b) for the linear model `y = mx + b`

**Cost Function** (Mean Squared Error):
```
J(m,b) = (1/n) × Σ(y_actual - y_predicted)²
```

**Gradient Descent Updates**:
```
m = m - α × ∂J/∂m
b = b - α × ∂J/∂b

where:
∂J/∂m = -(2/n) × Σ[x × (y_actual - y_predicted)]
∂J/∂b = -(2/n) × Σ(y_actual - y_predicted)
α = learning rate
```

**Algorithm**: Start with random parameters, compute gradients of the cost function, update parameters in the opposite direction of gradients (steepest descent), and repeat until convergence.

## Key Observations

✅ **Convergence Behavior**: For perfectly linear data (y = 2x + 3), the algorithm achieves near-zero cost, demonstrating proper implementation.

✅ **Learning Rate Impact**: At 0.08, the algorithm balances speed and stability. Higher values risk overshooting the minimum; lower values converge slowly.

✅ **Gradient Calculation**: Using vectorized operations (NumPy) makes gradient computation efficient compared to explicit loops.

✅ **Convex Optimization**: MSE for linear regression is convex, guaranteeing the algorithm finds the global minimum regardless of initialization.

✅ **Iteration Count**: 10,000 iterations ensures convergence even for varied datasets, though early stopping could optimize performance.

## Usage

```python
import numpy as np

x = np.array([1, 2, 3, 4, 5])
y = np.array([5, 7, 9, 11, 13])

gradient_descent(x, y)
```

**Expected Result**: m ≈ 2.0, b ≈ 3.0, cost ≈ 0.0

## Requirements

```bash
pip install numpy
```

## Future Enhancements

- Implement early stopping based on cost convergence threshold
- Add visualization of cost function over iterations
- Extend to multiple features (multivariate linear regression)
- Compare performance with analytical solution (Normal Equation)

---

*Part of ML from scratch series - Building fundamental algorithms to understand core concepts*
